{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frechet Inception Distance (FID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Feature Extraction:** FID uses a pre-trained Inception v3 network to extract features from both real and generated images. This network captures high-level representations of the images.\n",
    "\n",
    "2. **Distribution Comparison:** It assumes that the feature vectors for both real and generated images follow a multidimensional Gaussian distribution.\n",
    "\n",
    "3. **Statistical Moments:** FID calculates the mean and covariance of the feature distributions for both real and generated images.\n",
    "\n",
    "4. **Distance Calculation:** The Frechet distance between these two Gaussian distributions is then computed. This distance is defined as:\n",
    "\n",
    "$$FID = ||μ_r - μ_g||^2 + Tr(Σ_r + Σ_g - 2\\sqrt{Σ_r Σ_g})$$\n",
    "\n",
    "   Where:\n",
    "   - $μ_r$ and $μ_g$ are the mean feature vectors for real and generated images\n",
    "   - $Σ_r$ and $Σ_g$ are the covariance matrices for real and generated images\n",
    "   - Tr denotes the trace of a matrix (sum of diagonal elements)\n",
    "\n",
    "**Interpretation:** A lower FID score indicates that the generated images are more similar to the real images. A score of 0 would mean the two distributions are identical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments for FID calculation\n",
    "# generator = Instance of the GAN generator model\n",
    "# dataloader = DataLoader for real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Resize images to Inception v3 input size\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats ie we will also preprocess the data in same way as the original imagenet dataset that was used to train Inception v3\n",
    "])  # Define preprocessing steps for Inception v3 input\n",
    "\n",
    "def prepare_inception_input(images):\n",
    "    return preprocess(images)  # Apply preprocessing to the input images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Inception v3 model expects input images with the following characteristics:\n",
    "- Size: 299x299 pixels\n",
    "- Normalization: Using ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "- Tensor shape: (batch_size, 3, 299, 299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FID calculation, we're interested in the features from an intermediate layer of the Inception v3 model, not its final classification output. Typically, we use the output of the last pooling layer before the fully connected layers.\n",
    "To extract these features, we need to modify how we use the Inception model slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inception_model():\n",
    "    model = inception_v3(pretrained=True, transform_input=False)  # Load pre-trained Inception v3 model without input transformation\n",
    "    model.fc = torch.nn.Identity()  # Removes the last fully connected layer\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model.to(device)  # Move the model to the same device as our GAN\n",
    "\n",
    "inception_model = load_inception_model()  # Load and prepare the modified Inception v3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    with torch.no_grad():\n",
    "        features = inception_model(images)\n",
    "    return features  # This will be a tensor of shape [batch_size, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_real_features(dataloader, num_images=1000):\n",
    "    real_features = []  # List to store features of real images\n",
    "    image_count = 0  # Counter for processed images\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for batch in dataloader:\n",
    "            images = batch[0].to(device)  # Move batch of images to the device (assuming batch[0] contains images)\n",
    "            batch_size = images.size(0)  # Get the current batch size\n",
    "            \n",
    "            if image_count + batch_size > num_images:\n",
    "                # If adding this batch would exceed num_images, only take what's needed\n",
    "                images = images[:num_images - image_count]\n",
    "            \n",
    "            preprocessed_images = prepare_inception_input(images)  # Preprocess images for Inception v3\n",
    "            batch_features = extract_features(preprocessed_images)  # Extract features using Inception v3\n",
    "            \n",
    "            real_features.append(batch_features)  # Add batch features to the list\n",
    "            \n",
    "            image_count += batch_features.size(0)  # Update the count of processed images\n",
    "            if image_count >= num_images:\n",
    "                break  # Stop if we've processed enough images\n",
    "\n",
    "    real_features_tensor = torch.cat(real_features, dim=0)  # Concatenate all features into a single tensor\n",
    "    return real_features_tensor[:num_images]  # Return exactly num_images features\n",
    "\n",
    "# Extract features from 1000 real images\n",
    "real_features = extract_real_features(dataloader, num_images=1000)  # Extract features from 1000 real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and covariance of real features\n",
    "real_mean = torch.mean(real_features, dim=0)  # Calculate mean across all samples for each feature\n",
    "real_cov = torch.cov(real_features.T)  # Calculate covariance matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_extract_features(generator, num_samples=1000, batch_size=64):\n",
    "    generated_features = []  # List to store features of generated images\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # Calculate number of batches needed\n",
    "\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "    print(f\"Starting feature extraction for {num_samples} samples with batch size {batch_size}\")  # Print start of extraction process\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for batch_idx in range(num_batches):\n",
    "            current_batch_size = min(batch_size, num_samples - len(generated_features))  # Adjust batch size for last batch if needed\n",
    "            print(f\"Processing batch {batch_idx + 1}/{num_batches} with size {current_batch_size}\")  # Print current batch information\n",
    "            noise = torch.randn(current_batch_size, 100, 1, 1, device=device)  # Generate noise for input to generator\n",
    "            fake_images = generator(noise)  # Generate fake images\n",
    "            \n",
    "            preprocessed_images = prepare_inception_input(fake_images)  # Preprocess images for Inception v3\n",
    "            batch_features = extract_features(preprocessed_images)  # Extract features using Inception v3\n",
    "            \n",
    "            generated_features.append(batch_features)  # Add batch features to the list\n",
    "            print(f\"Extracted features shape: {batch_features.shape}\")  # Print shape of extracted features\n",
    "            \n",
    "            if len(generated_features) * batch_size >= num_samples:\n",
    "                print(f\"Reached target number of samples. Stopping extraction.\")  # Print when target samples reached\n",
    "                break  # Stop if we've generated enough samples\n",
    "    print(f\"Feature extraction completed. Total features extracted: {len(generated_features) * batch_size}\")  # Print completion of extraction process\n",
    "\n",
    "    generated_features_tensor = torch.cat(generated_features, dim=0)  # Concatenate all features into a single tensor\n",
    "    return generated_features_tensor[:num_samples]  # Return exactly num_samples features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 samples and extract their features\n",
    "generated_features = generate_and_extract_features(generator, num_samples=1000)  # Generate and extract features from 1000 fake images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and covariance of generated features\n",
    "generated_mean = torch.mean(generated_features, dim=0)  # Calculate mean across all samples for each feature\n",
    "generated_cov = torch.cov(generated_features.T)  # Calculate covariance matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_inception_distance(real_mean, real_cov, generated_mean, generated_cov):\n",
    "    \"\"\"\n",
    "    Calculate the Fréchet Inception Distance (FID) between real and generated image features.\n",
    "    \n",
    "    Args:\n",
    "    real_mean (torch.Tensor): Mean of real image features.\n",
    "    real_cov (torch.Tensor): Covariance matrix of real image features.\n",
    "    generated_mean (torch.Tensor): Mean of generated image features.\n",
    "    generated_cov (torch.Tensor): Covariance matrix of generated image features.\n",
    "    \n",
    "    Returns:\n",
    "    float: The calculated FID score.\n",
    "    \"\"\"\n",
    "    import torch  # Import torch for tensor operations\n",
    "    from scipy import linalg  # Import linalg for matrix operations\n",
    "    \n",
    "    # Convert to numpy for scipy operations\n",
    "    real_mean_np = real_mean.cpu().numpy()  # Convert real mean to numpy array\n",
    "    real_cov_np = real_cov.cpu().numpy()  # Convert real covariance to numpy array\n",
    "    generated_mean_np = generated_mean.cpu().numpy()  # Convert generated mean to numpy array\n",
    "    generated_cov_np = generated_cov.cpu().numpy()  # Convert generated covariance to numpy array\n",
    "    \n",
    "    # Calculate squared L2 norm between means\n",
    "    mean_diff = np.sum((real_mean_np - generated_mean_np) ** 2)  # Compute squared difference between means\n",
    "    \n",
    "    # Calculate sqrt of product of covariances\n",
    "    covmean = linalg.sqrtm(real_cov_np.dot(generated_cov_np))  # Compute matrix square root\n",
    "    \n",
    "    # Check and correct imaginary parts if necessary\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real  # Take only the real part if result is complex\n",
    "    \n",
    "    # Calculate trace term\n",
    "    trace_term = np.trace(real_cov_np + generated_cov_np - 2 * covmean)  # Compute trace of the difference\n",
    "    \n",
    "    # Compute FID\n",
    "    fid = mean_diff + trace_term  # Sum up mean difference and trace term\n",
    "    \n",
    "    return fid  # Return FID as a Python float\n",
    "\n",
    "# Calculate FID using the improved function\n",
    "fid_score = calculate_frechet_inception_distance(real_mean, real_cov, generated_mean, generated_cov)  # Compute FID score\n",
    "print(f\"Fréchet Inception Distance (FID): {fid_score:.4f}\")  # Print the calculated FID score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForADRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
